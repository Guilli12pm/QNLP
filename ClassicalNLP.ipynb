{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bag-O-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.864406779661017\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "train_dataset = pd.DataFrame({\"Sentence\": train_dataset.iloc[:, 0].to_list(), \"Type\": train_dataset.iloc[:, 1].to_list()})\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.DataFrame({\"Sentence\": test_dataset.iloc[:, 0].to_list(), \"Type\": test_dataset.iloc[:, 1].to_list()})\n",
    "\n",
    "# Tokenize the sentences\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_dataset['Sentence'])\n",
    "y_train = train_dataset['Type']\n",
    "\n",
    "# Train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "X_test = vectorizer.transform(test_dataset['Sentence'])\n",
    "y_test = test_dataset['Type']\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Term Frequency-Inverse Document Frequency (Tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8813559322033898\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "train_dataset = pd.DataFrame({\"Sentence\": train_dataset.iloc[:, 0].to_list(), \"Type\": train_dataset.iloc[:, 1].to_list()})\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.DataFrame({\"Sentence\": test_dataset.iloc[:, 0].to_list(), \"Type\": test_dataset.iloc[:, 1].to_list()})\n",
    "\n",
    "# Tokenize the sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_dataset['Sentence'])\n",
    "y_train = train_dataset['Type']\n",
    "\n",
    "# Train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "X_test = vectorizer.transform(test_dataset['Sentence'])\n",
    "y_test = test_dataset['Type']\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8153846153846154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "# Sample dataset\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.25, random_state=None)\n",
    "\n",
    "# Vectorize the text data using HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False)\n",
    "X_train = vectorizer.transform(docs_train)\n",
    "X_test = vectorizer.transform(docs_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bag-O-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "# Sample dataset\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer and logistic regression classifier pipeline\n",
    "pipeline = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the testing set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6307692307692307\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "# Sample dataset\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer and logistic regression classifier pipeline\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the testing set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bag-O-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7076923076923077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and RandomForest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', CountVectorizer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the testing set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7230769230769231\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and RandomForest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the testing set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6291394233703613\n",
      "Epoch 2, Loss: 0.6717043519020081\n",
      "Epoch 3, Loss: 0.5689048171043396\n",
      "Epoch 4, Loss: 0.6689860820770264\n",
      "Epoch 5, Loss: 0.7063190340995789\n",
      "Finished Training\n",
      "Accuracy: 0.5576923076923077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv(\"dataset_dim2/train_clean_data.csv\").sample(frac=1)\n",
    "test_dataset = pd.read_csv(\"dataset_dim2/test_clean_data.csv\").sample(frac=1)\n",
    "texts = train_dataset.iloc[:, 0].to_list() + test_dataset.iloc[:, 0].to_list()\n",
    "labels = train_dataset.iloc[:, 1].to_list() + test_dataset.iloc[:, 1].to_list()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "features = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test, y_train, y_test = map(torch.tensor, (X_train, X_test, np.array(y_train), np.array(y_test)))\n",
    "\n",
    "# Define a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "# Create test DataLoader\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "model = SentimentClassifier(features.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# You can add code here for validation/testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
